# Code Changes Summary: Current vs Enhanced Implementation

## Overview
This document outlines the specific changes needed to transform your current reactive AI DevOps agent into a production-ready, proactive AIOps system with workflow and mechanism integration.

---

## File-by-File Changes

### 1. **Orchestrator Lambda** (`05-orchestration/lambda/index.py`)

#### Current Implementation (221 lines)
- Simple event classification (FAILURE/TAMPERING/NORMAL)
- Direct CodeBuild trigger
- Basic SNS notifications
- No state management
- No historical context

#### Enhanced Implementation (`index_enhanced.py` - 650+ lines)
**Key Additions:**

```python
# NEW: Correlation IDs for tracking
correlation_id = generate_correlation_id()

# NEW: Structured logging
structured_log("INFO", "Handler invoked", correlation_id, event=event)

# NEW: Cooldown check to prevent recovery loops
in_cooldown, last_incident_id = check_cooldown(resource_type, resource_id)

# NEW: Historical context retrieval
similar_incidents = get_similar_incidents(resource_type, EventClassification.FAILURE)

# NEW: Enhanced Bedrock analysis with context
analysis = analyze_with_bedrock_enhanced(
    event_details=detail,
    resource_type=resource_type,
    resource_id=resource_id,
    similar_incidents=similar_incidents,
    correlation_id=correlation_id
)

# NEW: Confidence threshold check
if confidence < CONFIDENCE_THRESHOLD:
    # Request manual review instead of auto-recovery

# NEW: Multi-stage recovery planning
recovery_plan = generate_recovery_plan(
    analysis=analysis,
    resource_type=resource_type,
    resource_id=resource_id,
    similar_incidents=similar_incidents,
    correlation_id=correlation_id
)

# NEW: Workflow state tracking
update_workflow_state(correlation_id, WorkflowState.EXECUTING)

# NEW: CloudWatch metrics
publish_metrics(resource_type, classification, recovery_duration, success)
```

**Migration Path:**
1. **Phase 1**: Add DynamoDB incident table
2. **Phase 2**: Add correlation IDs and structured logging (non-breaking)
3. **Phase 3**: Add cooldown logic (prevents issues)
4. **Phase 4**: Enhance Bedrock prompts with historical context
5. **Phase 5**: Add confidence thresholds and manual review
6. **Phase 6**: Replace current `index.py` with `index_enhanced.py`

---

### 2. **NEW: DynamoDB Tables** (`05-orchestration/dynamodb.tf`)

**Purpose:** Store incident history for learning and pattern recognition

**Tables:**
1. **`aiops-incidents`**: Main incident tracking
   - Primary Key: `incident_id`
   - GSI 1: `resource-type-index` (for querying by resource type)
   - GSI 2: `resource-timestamp-index` (for cooldown checks)
   - GSI 3: `workflow-state-index` (for monitoring)

2. **`aiops-patterns`**: Pattern recognition
   - Primary Key: `pattern_id`
   - GSI: `resource-pattern-index`

**Schema Example:**
```json
{
  "incident_id": "incident-uuid-here",
  "incident_timestamp": "2025-12-06T15:00:00Z",
  "resource_type": "ec2",
  "resource_id": "i-1234567890",
  "resource_key": "ec2#i-1234567890",
  "workflow_state": "COMPLETED",
  "event_classification": "FAILURE",
  "confidence": 0.95,
  "severity": 8,
  "llm_analysis": "{...}",
  "recovery_plan": "{...}",
  "recovery_result": "{...}",
  "recovery_duration_seconds": 35,
  "success": true,
  "created_at": "2025-12-06T15:00:00Z",
  "updated_at": "2025-12-06T15:00:35Z"
}
```

**Deployment:**
```bash
cd aiops-devops-agent/05-orchestration
terraform init
terraform plan
terraform apply
```

---

### 3. **NEW: Step Functions Workflow** (`05-orchestration/workflow_state_machine.json`)

**Purpose:** Orchestrate multi-stage recovery with retry logic and error handling

**Workflow Stages:**
```
DetectIncident
    ‚Üì
AnalyzeWithBedrock
    ‚Üì
CheckSeverity ‚Üí (Choice)
    ‚îú‚îÄ NORMAL ‚Üí LogNormalEvent ‚Üí End
    ‚îú‚îÄ ANOMALY ‚Üí NotifyAnomaly ‚Üí End
    ‚îú‚îÄ Low Confidence ‚Üí RequestManualReview ‚Üí End
    ‚îî‚îÄ FAILURE/TAMPERING ‚Üí GenerateRecoveryPlan
                                ‚Üì
                          ExecuteRecovery (Parallel)
                            ‚îú‚îÄ RestoreInfrastructure (CodeBuild)
                            ‚îî‚îÄ UpdateIncidentRecord (DynamoDB)
                                ‚Üì
                          VerifyRecovery
                                ‚Üì
                          CheckVerificationResult ‚Üí (Choice)
                            ‚îú‚îÄ VERIFIED ‚Üí NotifySuccess ‚Üí StoreSuccessMetrics
                            ‚îú‚îÄ PARTIAL ‚Üí NotifyPartialSuccess ‚Üí StoreSuccessMetrics
                            ‚îî‚îÄ FAILED ‚Üí HandleRecoveryFailure ‚Üí StoreFailureMetrics
```

**Benefits:**
- Visual workflow monitoring in AWS Console
- Automatic retries with exponential backoff
- Parallel execution (infrastructure + logging)
- Built-in error handling
- State persistence

**Terraform Integration:**
```hcl
resource "aws_sfn_state_machine" "aiops_recovery_workflow" {
  name     = "aiops-recovery-workflow"
  role_arn = aws_iam_role.step_functions_role.arn

  definition = templatefile("${path.module}/workflow_state_machine.json", {
    orchestrator_lambda_arn  = aws_lambda_function.orchestrator.arn
    verification_lambda_arn  = aws_lambda_function.verification.arn
    incidents_table_name     = aws_dynamodb_table.aiops_incidents.name
    sns_topic_arn           = var.sns_topic_arn
    codebuild_project_name  = var.codebuild_project_name
  })
}
```

---

### 4. **NEW: Proactive Log Analyzer** (`06-log-analyzer/lambda/index.py`)

**Purpose:** Analyze CloudWatch Logs to predict failures BEFORE they occur

**Key Features:**

```python
# 1. Extract recent logs using CloudWatch Logs Insights
log_results = extract_log_insights(log_group, hours_back=1)

# 2. Extract error patterns
current_patterns = extract_error_patterns(log_messages)
# Example: {'ERROR': 45, 'Exception': 12, 'timeout': 8}

# 3. Compare to historical baseline (stored in DynamoDB)
anomalies = detect_anomalies(current_patterns, log_group)
# Example: [{'pattern': 'timeout', 'z_score': 3.5, 'severity': 8}]

# 4. Semantic analysis with Bedrock
analysis = analyze_logs_with_bedrock(log_messages, anomalies)
# Returns: {
#   'failure_probability': 0.75,
#   'root_cause': 'Database connection pool exhaustion',
#   'recommended_action': 'Scale up RDS instance',
#   'urgency': 'HIGH'
# }

# 5. Send proactive alert if failure probability > threshold
if failure_probability > 0.7:
    send_proactive_alert(log_group, analysis, anomalies)
```

**Trigger:** EventBridge scheduled rule (every 5 minutes)

**Example Alert:**
```
Subject: üîÆ AIOps: Proactive Alert - HIGH - /aws/lambda/my-app

Failure Probability: 75%
Root Cause: Database connection pool exhaustion
Recommended Action: Scale up RDS instance

This is a PROACTIVE alert. The system has not failed yet.
```

---

### 5. **NEW: Verification Lambda** (`07-verification/lambda/index.py`)

**Purpose:** Verify that recovery was successful

**Implementation:**
```python
def verify_ec2_recovery(instance_id):
    """Verify EC2 instance is running and healthy"""
    ec2 = boto3.client('ec2')
    response = ec2.describe_instances(InstanceIds=[instance_id])
    state = response['Reservations'][0]['Instances'][0]['State']['Name']
    return state == 'running'

def verify_lambda_recovery(function_name):
    """Verify Lambda function exists and is active"""
    lambda_client = boto3.client('lambda')
    try:
        response = lambda_client.get_function(FunctionName=function_name)
        return response['Configuration']['State'] == 'Active'
    except lambda_client.exceptions.ResourceNotFoundException:
        return False

def handler(event, context):
    resource_type = event['resource_type']
    resource_id = event['resource_id']
    
    if resource_type == 'ec2':
        verified = verify_ec2_recovery(resource_id)
    elif resource_type == 'lambda':
        verified = verify_lambda_recovery(resource_id)
    # ... other resource types
    
    return {
        'status': 'VERIFIED' if verified else 'FAILED',
        'resource_type': resource_type,
        'resource_id': resource_id,
        'duration': (datetime.utcnow() - start_time).total_seconds()
    }
```

---

## Infrastructure Changes

### Current Infrastructure
```
EventBridge ‚Üí Orchestrator Lambda ‚Üí CodeBuild ‚Üí Terraform
                      ‚Üì
                    SNS
```

### Enhanced Infrastructure
```
EventBridge ‚Üí Orchestrator Lambda ‚Üí Step Functions State Machine
                      ‚Üì                        ‚Üì
                  DynamoDB              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  (Incidents)           ‚Üì             ‚Üì
                                   CodeBuild    Verification Lambda
                                        ‚Üì             ‚Üì
                                   Terraform      DynamoDB
                                        ‚Üì             ‚Üì
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚Üì
                                             SNS

CloudWatch Logs ‚Üí Log Analyzer Lambda ‚Üí Bedrock ‚Üí SNS (Proactive Alerts)
                         ‚Üì
                    DynamoDB
                    (Patterns)
```

---

## Environment Variables Changes

### Current
```python
SNS_TOPIC_ARN = os.environ['SNS_TOPIC_ARN']
MODEL_ID = "amazon.titan-text-express-v1"
```

### Enhanced
```python
# Existing
SNS_TOPIC_ARN = os.environ['SNS_TOPIC_ARN']
MODEL_ID = "amazon.titan-text-express-v1"

# NEW
INCIDENT_TABLE = os.environ.get('INCIDENT_TABLE', 'aiops-incidents')
PATTERNS_TABLE = os.environ.get('PATTERNS_TABLE', 'aiops-patterns')
STATE_MACHINE_ARN = os.environ.get('STATE_MACHINE_ARN', '')
COOLDOWN_MINUTES = int(os.environ.get('COOLDOWN_MINUTES', '5'))
CONFIDENCE_THRESHOLD = float(os.environ.get('CONFIDENCE_THRESHOLD', '0.8'))
ANOMALY_THRESHOLD = float(os.environ.get('ANOMALY_THRESHOLD', '0.7'))
LOG_GROUPS = os.environ.get('LOG_GROUPS', '').split(',')
```

---

## IAM Permissions Changes

### Current Permissions
```json
{
  "Effect": "Allow",
  "Action": [
    "bedrock:InvokeModel",
    "codebuild:StartBuild",
    "sns:Publish"
  ],
  "Resource": "*"
}
```

### Enhanced Permissions
```json
{
  "Effect": "Allow",
  "Action": [
    "bedrock:InvokeModel",
    "codebuild:StartBuild",
    "sns:Publish",
    
    // NEW: DynamoDB access
    "dynamodb:PutItem",
    "dynamodb:GetItem",
    "dynamodb:UpdateItem",
    "dynamodb:Query",
    "dynamodb:Scan",
    
    // NEW: Step Functions
    "states:StartExecution",
    "states:DescribeExecution",
    
    // NEW: CloudWatch Logs Insights
    "logs:StartQuery",
    "logs:GetQueryResults",
    "logs:DescribeLogGroups",
    
    // NEW: CloudWatch Metrics
    "cloudwatch:PutMetricData",
    
    // NEW: EC2/Lambda verification
    "ec2:DescribeInstances",
    "lambda:GetFunction",
    "dynamodb:DescribeTable",
    "s3:HeadBucket"
  ],
  "Resource": "*"
}
```

---

## Deployment Strategy

### Option 1: Gradual Migration (Recommended)
```bash
# Week 1: Add DynamoDB tables (no code changes)
cd aiops-devops-agent/05-orchestration
terraform apply -target=aws_dynamodb_table.aiops_incidents
terraform apply -target=aws_dynamodb_table.aiops_patterns

# Week 2: Deploy enhanced Lambda alongside current (blue/green)
# Update Lambda code to index_enhanced.py
# Test with sample events
# Monitor for 1 week

# Week 3: Add Step Functions workflow
terraform apply -target=aws_sfn_state_machine.aiops_recovery_workflow

# Week 4: Deploy log analyzer
cd ../06-log-analyzer
terraform apply

# Week 5: Switch EventBridge to trigger Step Functions instead of Lambda
# Week 6: Decommission old Lambda
```

### Option 2: Fresh Deployment
```bash
# Deploy everything at once (for new environments)
cd aiops-devops-agent
terraform init
terraform plan
terraform apply
```

---

## Testing Checklist

### Unit Tests
- [ ] Test cooldown logic
- [ ] Test confidence threshold
- [ ] Test pattern recognition
- [ ] Test Bedrock prompt parsing
- [ ] Test workflow state transitions

### Integration Tests
- [ ] Test EC2 termination ‚Üí detection ‚Üí recovery ‚Üí verification
- [ ] Test Lambda deletion ‚Üí detection ‚Üí recovery ‚Üí verification
- [ ] Test SSM tampering ‚Üí detection ‚Üí recovery ‚Üí verification
- [ ] Test log anomaly ‚Üí proactive alert
- [ ] Test Step Functions workflow end-to-end

### Load Tests
- [ ] 100 incidents in 1 hour (cooldown should prevent loops)
- [ ] 1000 log messages analyzed
- [ ] DynamoDB query performance

---

## Monitoring & Observability

### New CloudWatch Dashboards

**1. Incident Dashboard**
- Incidents per hour (by resource type)
- Recovery success rate
- Average recovery duration
- Workflow state distribution

**2. Log Analyzer Dashboard**
- Anomalies detected per hour
- Failure probability trend
- Proactive alerts sent
- Pattern baseline drift

**3. AI Performance Dashboard**
- Bedrock invocation count
- Bedrock latency
- Classification accuracy (manual validation)
- Confidence score distribution

### Alarms
```hcl
resource "aws_cloudwatch_metric_alarm" "high_failure_rate" {
  alarm_name          = "aiops-high-failure-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "RecoveryFailure"
  namespace           = "AIOps/DevOpsAgent"
  period              = "300"
  statistic           = "Sum"
  threshold           = "3"
  alarm_description   = "Alert if more than 3 recovery failures in 10 minutes"
  alarm_actions       = [var.sns_topic_arn]
}
```

---

## Cost Comparison

### Current Monthly Cost: < $1

### Enhanced Monthly Cost: ~$6-8

**Breakdown:**
- Lambda invocations: $0.20 (within free tier)
- DynamoDB: $1.25 (25 GB, on-demand)
- Step Functions: $2.00 (100 executions)
- CloudWatch Logs Insights: $0.50 (5 GB analyzed)
- Bedrock (enhanced prompts): $2.00 (2x current usage)
- CloudWatch Metrics: $0.30
- X-Ray (optional): $0.50

**ROI:** Prevents even one 1-hour outage ‚Üí saves thousands in lost revenue

---

## Migration Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| DynamoDB throttling | Recovery delays | Use on-demand billing mode |
| Step Functions timeout | Recovery failure | Set appropriate timeouts, add retries |
| Bedrock rate limits | Analysis failure | Add exponential backoff, caching |
| False positive alerts | Alert fatigue | Tune confidence/anomaly thresholds |
| Recovery loops | Resource thrashing | Cooldown period (5 minutes) |
| High costs | Budget overrun | Set billing alarms, use free tier |

---

## Success Metrics

### Before (Current)
- ‚úÖ Recovery time: ~35 seconds
- ‚úÖ Detection: Real-time (< 1s)
- ‚ùå No learning from history
- ‚ùå No proactive detection
- ‚ùå No audit trail

### After (Enhanced)
- ‚úÖ Recovery time: ~25 seconds (faster due to parallel execution)
- ‚úÖ Detection: Real-time (< 1s)
- ‚úÖ Learning from history (pattern recognition)
- ‚úÖ Proactive detection (5-minute intervals)
- ‚úÖ Full audit trail (DynamoDB)
- ‚úÖ 95%+ recovery success rate
- ‚úÖ 30%+ failures prevented proactively

---

## Next Steps

1. **Review this document** and the generated code files
2. **Decide on deployment strategy** (gradual vs fresh)
3. **Create DynamoDB tables** first (non-breaking change)
4. **Test enhanced Lambda** in development environment
5. **Deploy Step Functions workflow**
6. **Deploy log analyzer** for proactive monitoring
7. **Monitor and tune** thresholds based on real data
8. **Iterate and improve** based on feedback

---

## Questions to Consider

1. **Which log groups should the log analyzer monitor?**
   - Application logs: `/aws/lambda/*`
   - ECS logs: `/ecs/*`
   - RDS logs: `/aws/rds/*`

2. **What confidence threshold should trigger auto-recovery?**
   - Recommended: 0.8 (80%)
   - Conservative: 0.9 (90%)
   - Aggressive: 0.7 (70%)

3. **How long should the cooldown period be?**
   - Recommended: 5 minutes
   - For critical resources: 10 minutes
   - For non-critical: 2 minutes

4. **Should Step Functions be synchronous or asynchronous?**
   - Synchronous: EventBridge ‚Üí Step Functions (wait for completion)
   - Asynchronous: EventBridge ‚Üí Lambda ‚Üí Step Functions (fire and forget)
   - Recommended: Asynchronous for faster response

5. **How long to retain incident history?**
   - Recommended: 90 days (with DynamoDB TTL)
   - For compliance: 1 year
   - For cost savings: 30 days
